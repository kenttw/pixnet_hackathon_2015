{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import jieba, os, re, string\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "# from utils.utils import *\n",
    "import pandas as pd\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "testmode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Built-in dictionary for cutting words\n",
    "path_dict_for_tokenization = '/Users/easonchan/git/jieba/extra_dict/dict.txt.big'\n",
    "path_dict_for_stopwords = '/Users/easonchan/Desktop/stop words freq list'\n",
    "# Path of corpora\n",
    "#path_corpora = '/Users/easonchan/temp/thenewslense'\n",
    "#path_corpora = '/Users/easonchan/Desktop/xmlOutput'\n",
    "# path_corpora = '/Users/easonchan/temp/UDNnews'  ## only 3k articles under UDN news\n",
    "path_corpora = '/Users/easonchan/temp/iterativeUDNnews' ##20k articles under UDN news\n",
    "\n",
    "## Hyperparameters for training model\n",
    "# Minimun length of single document\n",
    "min_length = 300 if not 'UDNnews' in path_corpora else 150\n",
    "# Num_topics in LDA\n",
    "#num_topics = 40 if not 'UDNnews' in path_corpora else 41 #=> little bit better than 55\n",
    "num_topics = 40 if not 'UDNnews' in path_corpora else 100 #=> little bit better than 55\n",
    "# Filter out tokens that appear in less than `no_below` documents (absolute number)\n",
    "no_below_this_number = 20 if not 'UDNnews' in path_corpora else 15\n",
    "# Filter out tokens that appear in more than `no_above` documents (fraction of total corpus size, *not* absolute number).\n",
    "no_above_fraction_of_doc = 0.15\n",
    "# Granularity of model, may increase the model complexity, default is false\n",
    "full_mode = False \n",
    "# Use HMM model for tokenization, default is true\n",
    "HMM_mode_on = True\n",
    "# Remove topic which weights less than this number\n",
    "remove_topic_so_less = 0.08\n",
    "# Number of iterations in training LDA model, the less the documents in total, the more the iterations for LDA model to converge\n",
    "num_of_iterations = 30 if testmode else 200\n",
    "# Number of passes in the model\n",
    "passes = 5 if testmode else 3\n",
    "\n",
    "#Print all hyperparameters\n",
    "parameters = {}\n",
    "parameters['testmode'] = testmode\n",
    "parameters['min_length'] = min_length\n",
    "parameters['num_topics'] = num_topics\n",
    "parameters['no_below_this_number'] = no_below_this_number\n",
    "parameters['full_mode'] = full_mode\n",
    "parameters['HMM_mode_on'] = HMM_mode_on\n",
    "parameters['no_above_fraction_of_doc'] = no_above_fraction_of_doc\n",
    "parameters['remove_topic_so_less'] = remove_topic_so_less\n",
    "parameters['num_of_iterations'] = num_of_iterations\n",
    "parameters['passes'] = passes\n",
    "for k in parameters:\n",
    "    print \"Parameter for {0} is {1}\".format(k,parameters[k])\n",
    "\n",
    "# get stop words dict\n",
    "stopwords = get_stop_words_list(path_dict_for_stopwords)\n",
    "jieba.set_dictionary(path_dict_for_tokenization)\n",
    "walk = os.walk(path_corpora)\n",
    "doc_count=0\n",
    "train_set = []\n",
    "doc_mapping={}\n",
    "\n",
    "## Start of preparing list of documents and tokens [[words_in_1st_doc],[words_in_2nd_doc]....]\n",
    "for root, dirs, files in walk:\n",
    "    for name in files:        \n",
    "        f = open(os.path.join(root, name), 'r')\n",
    "        raw = f.read()\n",
    "        f.close()\n",
    "        # call preprocessing function\n",
    "        preprocessed_text = preprocessing(raw)\n",
    "        # Skip document length < min_length\n",
    "        if len(preprocessed_text) < min_length or name=='.DS_Store':\n",
    "            continue\n",
    "        tokens = tokenize(preprocessed_text,stopwords,full_mode,HMM_mode_on)\n",
    "        train_set.append(list(tokens))\n",
    "        \n",
    "        # Build doc-mapping\n",
    "        doc_mapping[doc_count] = name\n",
    "        doc_count = doc_count+1\n",
    "\n",
    "print 'There are %i documents in the pool' % (doc_count)\n",
    "## End of preparation \n",
    "\n",
    "## Start of preparing corpus of docs, which comprise Bag-Of-Words (BOW)\n",
    "dic = corpora.Dictionary(train_set)\n",
    "print \"In the corpus there are\", len(dic), \"raw tokens\"\n",
    "denominator = len(dic)\n",
    "dic.filter_extremes(no_below=no_below_this_number, no_above=no_above_fraction_of_doc)\n",
    "nominator = len(dic)\n",
    "print \"After filtering, in the corpus there are\", len(dic), \"unique tokens, reduced \", (1-(nominator/denominator)),\"%\"\n",
    "corpus = [dic.doc2bow(text) for text in train_set]\n",
    "## End of preparation \n",
    "\n",
    "## Implementing TF-IDF as vector for each document, and train LDA model on top of that\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "lda = models.LdaModel(corpus_tfidf, id2word = dic, num_topics = num_topics,iterations=num_of_iterations,passes = passes)\n",
    "corpus_lda = lda[corpus_tfidf]\n",
    "\n",
    "\n",
    "## Exhibit all the topics and related words\n",
    "for i in range(num_topics):\n",
    "    print 'Topic %s : ' % (str(i)) + lda.print_topic(i)\n",
    "\n",
    "\n",
    "## Exhibit visualization of LDA model in interactive graphic\n",
    "#vis = pyLDAvis.gensim.prepare(lda, corpus, dic)\n",
    "#pyLDAvis.display(vis)\n",
    "\n",
    "## Exhibit perplexity of current model under specific topic hyperparameter : k\n",
    "print '==============================='\n",
    "print 'Model perplexity : ',lda.bound(corpus_lda)\n",
    "print '==============================='\n",
    "\n",
    "## Save model output\n",
    "save_path = '/Users/easonchan/Desktop/LDAmodel/final_ldamodel'\n",
    "print 'Model saved at {0}'.format(save_path)\n",
    "lda.save(save_path)\n",
    "\n",
    "## Save corpus output\n",
    "save_path = '/Users/easonchan/Desktop/LDAmodel/corpus.pickle'\n",
    "mappingFile = open(save_path,'w')\n",
    "pickle.dump(corpus,mappingFile)\n",
    "mappingFile.close()\n",
    "print 'Corpus saved at {0}'.format(save_path)\n",
    "\n",
    "# Save document mapping\n",
    "path_mappingfile= '/Users/easonchan/Desktop/LDAmodel/documentmapping.pickle'\n",
    "mappingFile = open(path_mappingfile,'w')\n",
    "pickle.dump(doc_mapping,mappingFile)\n",
    "mappingFile.close()\n",
    "print 'Document mapping saved at {0}'.format(path_mappingfile)\n",
    "\n",
    "# Save doc to topic matrix\n",
    "doc_topic_matrix = {}\n",
    "count = 0\n",
    "for doc in corpus:\n",
    "    dense_vector = {}\n",
    "    vector = convertListToDict(lda[doc])\n",
    "    # remove topic that is so irrelevant\n",
    "    for topic in vector:\n",
    "        if vector[topic] > remove_topic_so_less:\n",
    "            dense_vector[topic] = vector[topic]\n",
    "    doc_topic_matrix[count]=dense_vector\n",
    "    count = count+1\n",
    "savePickleFile('doc_topic_matrix',doc_topic_matrix)\n",
    "print 'doc to topic mapping saved at {0}'.format('doc_topic_matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
